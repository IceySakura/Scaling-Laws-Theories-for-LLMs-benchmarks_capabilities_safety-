### **Efficient Benchmarking (of Language Models)**

讨论如何设计**高效**的 Benchmark

关注问题：computation-**reliability** trade-off

细化定义：

> *Reliability* assesses the degree to which the evaluation answer remains **consistent** under different random **decisions**, 
>
> many of which are selections from the distribution of elements composing the benchmark.

reliability 指不同 decision 下评分是否相似

decision 比如数据集的选择，数据的划分，不同的 Scenarios...

Decision Impact on Reliability(*DIoR*)
$$
DIoR = CI_{95\%,c \sim D}(f(s_{c_o}(M),s_c(M)))
$$
$M$ 是一组模型

$c \sim D$ 是随机实例化的一个 Decision

$c_o$ 是初始的 Decision（要评估的 Decision）

$s$ 是评分函数，又称为 metric

$f$ 用于评判相似度，值域 $[0,1]$，又称为 meta-metric

$CI_{95\%}$ 95% 置信度的置信区间，取下界作为 DIoR 的值

---

具体的例子：Flash-HELM

motivation: meta-metric 的特殊性质

> For example, when a model’s ranking falls within the lower range of the benchmark – say, between positions 25 and 40, the precise ranking might not hold much importance; instead, a broad conclusion that the model is poor should suffice. 
>
> On the flip side, when a model attains a position in the top 5 ranks, the specific placement carries more weight.

1. 评分函数为排名，先分梯队，再在梯队内排名。

   Rank 1-4, 5-9, 10-19, 20-50.

2. 对不同梯队，设置不同的 rank resolution，由高到低。

![image-20241105202347915](.\image\image3.png)

计算量和排名的确信区间的关系：

![image-20241105202248435](.\image\image4.png)

先粗筛，再细筛。

如果当前 Sample size S ，已经满足一个模型所在的梯队的 rank resolution，

就直接 report ，不再参与后续测试。

![image-20241105203704328](C:\Users\Singe\Desktop\Useful\Research\NLP\Scaling-Laws-Theories-for-LLMs-benchmarks_capabilities_safety-\areport\image\image5.png)

---

设计 benchmark 的建议

![image-20241105184203825](.\image\image2.png)

最大化数据点的多样性来提升可靠性。

不要对分数进行整合。（可能是指，会使得 reliability 虚高，不容易发现 benchmark 的问题）

|         | Bench1 | Bench2 |
| ------- | ------ | ------ |
| Task1   | 1      | 0.5    |
| Task2   | 0.5    | 1      |
| overall | 1.5    | 1.5    |

*尝试复现到 openeval*

### Scaling law

规模（模型、数据集） <-> 性能

数据污染，训练算法：相同规模下有更好的性能，如何影响性能。

多语言 Benchmark：对于多语言模型，单语言的 benchmark 不能很好反映性能。

[[2410.12883\] Scaling Laws for Multilingual Language Models](https://arxiv.org/abs/2410.12883)

不能单独测各个语言 benchmark 再加权求和，原因是跨语言迁移 cross-lingual transfer

> We introduce and validate a hypothesis that the test cross entropy loss for each language family is determined solely by its own sampling ratio, independent of other languages in the mixture.

假设：不同 language family 对 performance 的影响独立。

模型规模、数据集规模、sampling ratio <-> 性能
