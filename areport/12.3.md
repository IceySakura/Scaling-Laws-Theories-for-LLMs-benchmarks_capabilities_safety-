### 聚类->精选测试集

为什么聚类？—— 找到一组问题中，最具有代表性的问题，所谓锚点 "anchor points"。

聚类的重点是如何将数据点（问题），映射到空间里。

空间的每个维度反映问题对不同能力维度的考察程度/问题难度。

也可能反应问题本身的语义/主题等。

#### 从已有的模型评测结果角度

inspiration：不同模型的能力维度有别。

对于一个问题，将它在不同模型上的评测结果作为向量。

改进：修改建模思路，引入模型能力 $\alpha \in R^d$，问题考察维度 $\beta \in R^d$ 和 问题难度 $\gamma \in R$，由这些参数可以预测评测结果。

利用与实际结果之间的 loss 来优化求解上述参数。

#### 从 text 自身角度

使用 NLP 中已有的文本聚类技术。

1. 利用朴素的文本聚类 text clustering 中将文本表示为向量的技术，如 word2vec 等

2. 利用主题建模 topic modeling，假设每个文本是由若干个“主题”组合而成的，得到文本主题分布。将主题作为特征向量。

也有一些基于质量的考虑，比如文本中的拼写错误、单词冗余、词汇多样性等。倾向于选择质量更高的文本。

### 思考/计划

#### 从其他角度？

考虑评测的角度，比如专注于数学能力、道德观念、安全性等...

问题对某一能力的考查能力是否已经有可以量化的标准？

#### 后续关注评测的各个视角可能的特殊性质

在 evaluation 中可能关注的其他问题，作为知识补充和扩展。

比如一些特殊的评测角度下，问题具有特殊性质，可以提出新的方法以精选评测集。

#### 训练集->测试集

减小测试集？ 减小训练集？ *数据蒸馏*？

将大规模数据集蒸馏成很小的数据集，训练后能得到类似的参数。

数据蒸馏更多是一种合成，而非单纯的选择/采样，合成的数据可能完全无法被人类理解。

将数据蒸馏应用于 LLM Benchmark 存在的问题：

1. 数据蒸馏主要应用在图像，能找到的应用于文本的工作较少。
2. 人类无法理解的数据，该如何对模型的响应进行打分？（对响应的打分涉及到 label，看一下 CV 中的数据蒸馏是如何处理 label 的？）
3. 训练和评测的优化目标不同，训练是希望能得到类似的参数，评测是希望得到类似的评分/排名。（类似的特征，也许是一样的？）

不过提供新的 efficient 思路：高效的测试 -> 更小的测试集 -> 不一定从原测试集中选取，可以合成，但一定要可以评分（能跑）

而且那些（可以在训练集/测试集互通的）训练集中使用的选择/采样方法可能有借鉴意义。



主动学习 activate learning 

数据压缩 data reduction